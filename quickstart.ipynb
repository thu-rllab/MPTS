{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "JJl8CG4eVwNj",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $K$-shot sinusoid regression,  meta learners aim at quickly adapting the model to an unseen function $f(x)=a\\sin(x+b)$ with the help of $K$ data points randomly sampled from the function.\n",
    "This case treats the amplitude and phase variables $(a,b)$ as the task identifier to configure the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "AqnfI4QtVzZA"
   },
   "outputs": [],
   "source": [
    "#generating sinusoidal data\n",
    "class SineTask():\n",
    "    def __init__(self,amp,phase,min_x,max_x):\n",
    "        self.phase=phase\n",
    "        self.max_x=max_x\n",
    "        self.min_x=min_x\n",
    "        self.amp=amp\n",
    "\n",
    "    def sample_data(self,size=1):\n",
    "        x=np.random.uniform(self.max_x,self.min_x,size)\n",
    "        y=self.true_sine(x)\n",
    "        x=torch.tensor(x, dtype=torch.float).unsqueeze(1)\n",
    "        y=torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
    "        return x,y\n",
    "\n",
    "    def true_sine(self,x):\n",
    "        y=self.amp*np.sin(self.phase+x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "oek3HRLC3QK_"
   },
   "outputs": [],
   "source": [
    "class SineDistribution():\n",
    "    def __init__(self, min_amp, max_amp, min_phase, max_phase, min_x, max_x):\n",
    "        self.min_amp = min_amp\n",
    "        self.max_phase = max_phase\n",
    "        self.min_phase = min_phase\n",
    "        self.max_amp = max_amp\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "\n",
    "    def sample_task(self):\n",
    "        amp = np.random.uniform(self.min_amp, self.max_amp)\n",
    "        phase = np.random.uniform(self.min_phase, self.max_phase)\n",
    "        return SineTask(amp, phase, self.min_x, self.max_x)\n",
    "\n",
    "    def active_sample_task(self, amp, phase):\n",
    "        return SineTask(amp, phase, self.min_x, self.max_x)\n",
    "\n",
    "    def task_descriptor_candidate(self, num_candidate):\n",
    "        # grid search get 100 candidate tasks.\n",
    "        amp_list_candidate = np.random.uniform(self.min_amp, self.max_amp, size=[num_candidate, 1])\n",
    "        phase_list_candidate = np.random.uniform(self.min_phase, self.max_phase, size=[num_candidate, 1])\n",
    "        amp_list_candidate = np.float32(amp_list_candidate)\n",
    "        phase_list_candidate = np.float32(phase_list_candidate)\n",
    "        return amp_list_candidate, phase_list_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Meta Learner**\n",
    "\n",
    "Sine-net is the model's backbone with the parameter $\\theta$ (meta learner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "rtDNJDy3V1jj"
   },
   "outputs": [],
   "source": [
    "# defining our sine-net\n",
    "class SineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SineNet, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1, 40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40, 40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40, 1))\n",
    "        ]))\n",
    "\n",
    "    # We implemented argforward() so that we could use a set of custom weights for evaluation.\n",
    "    # This is important for the \"inner loop\" in MAML where you temporarily update the weights\n",
    "    # of the network for a task to calculate the meta-loss and then reset them for the next meta-task.\n",
    "\n",
    "    def argforward(self, x, weights):\n",
    "        x = F.linear(x, weights[0], weights[1])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, weights[2], weights[3])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, weights[4], weights[5])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RiskLearner:**\n",
    "\n",
    "We use basic architecture from\n",
    "https://github.com/EmilienDupont/neural-processes\n",
    "\n",
    "other options:\n",
    "https://github.com/automl/TransformersCanDoBayesianInference\n",
    "\n",
    "\n",
    "The risk learner is in an encoder-decoder structure, defined as $\\phi$ and $\\psi$ respectively.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functions in RiskLearner\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Maps an (x_i, y_i) pair to a representation r_i.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, h_dim, r_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.r_dim = r_dim\n",
    "\n",
    "        layers = [nn.Linear(x_dim + y_dim, h_dim),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Linear(h_dim, h_dim),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Linear(h_dim, r_dim)]\n",
    "\n",
    "        self.input_to_hidden = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        input_pairs = torch.cat((x, y), dim=1)\n",
    "        return self.input_to_hidden(input_pairs)\n",
    "\n",
    "\n",
    "class MuSigmaEncoder(nn.Module):\n",
    "    \"\"\"Maps r to z.\"\"\"\n",
    "\n",
    "    def __init__(self, r_dim, z_dim):\n",
    "        super(MuSigmaEncoder, self).__init__()\n",
    "\n",
    "        self.r_dim = r_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.r_to_hidden = nn.Linear(r_dim, r_dim)\n",
    "        self.hidden_to_mu = nn.Linear(r_dim, z_dim)\n",
    "        self.hidden_to_sigma = nn.Linear(r_dim, z_dim)\n",
    "\n",
    "    def forward(self, r):\n",
    "        hidden = torch.relu(self.r_to_hidden(r))\n",
    "        mu = self.hidden_to_mu(hidden)\n",
    "        sigma = 0.1 + 0.9 * torch.sigmoid(self.hidden_to_sigma(hidden))\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Maps (x+z) to y.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, h_dim, y_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.y_dim = y_dim\n",
    "\n",
    "        layers = [nn.Linear(x_dim + z_dim, h_dim),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Linear(h_dim, h_dim),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Linear(h_dim, h_dim),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.xz_to_hidden = nn.Sequential(*layers)\n",
    "        self.hidden_to_mu = nn.Linear(h_dim, y_dim)\n",
    "        self.hidden_to_sigma = nn.Linear(h_dim, y_dim)\n",
    "\n",
    "    def forward(self, x, z, output_type):\n",
    "        # batch_size=1\n",
    "\n",
    "        batch_size, num_points, _ = x.size()\n",
    "        num_repeat, _, _ = z.size()\n",
    "\n",
    "        if batch_size == 1:\n",
    "            x = x.repeat(num_repeat, 1, 1)\n",
    "            z = z.repeat(1, num_points, 1)\n",
    "\n",
    "            x_flat = x.view(num_repeat * num_points, self.x_dim)\n",
    "            z_flat = z.view(num_repeat * num_points, self.z_dim)\n",
    "\n",
    "            input_pairs = torch.cat((x_flat, z_flat), dim=1)\n",
    "            hidden = self.xz_to_hidden(input_pairs)\n",
    "            mu = self.hidden_to_mu(hidden)\n",
    "            pre_sigma = self.hidden_to_sigma(hidden)\n",
    "\n",
    "            mu = mu.view(num_repeat, num_points, self.y_dim)\n",
    "\n",
    "            if output_type == \"probabilistic\":\n",
    "                pre_sigma = pre_sigma.view(num_repeat, num_points, self.y_dim)\n",
    "                sigma = 0.1 + 0.9 * F.softplus(pre_sigma)\n",
    "                p_y_pred = Normal(mu, sigma)\n",
    "                p_y_pred = p_y_pred.rsample([1])[0]\n",
    "            else:\n",
    "                p_y_pred = mu\n",
    "\n",
    "            return p_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RiskLearner for the current task distribution and the current backbone parameters...\n",
    "\n",
    "class RiskLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements risklearner for functions of arbitrary dimensions.\n",
    "    x_dim : int Dimension of x values.\n",
    "    y_dim : int Dimension of y values.\n",
    "    r_dim : int Dimension of output representation r.\n",
    "    z_dim : int Dimension of latent variable z.\n",
    "    h_dim : int Dimension of hidden layer in encoder and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, r_dim, z_dim, h_dim):\n",
    "        super(RiskLearner, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.r_dim = r_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        # Initialize networks\n",
    "        self.xy_to_r = Encoder(x_dim, y_dim, h_dim, r_dim)\n",
    "        self.r_to_mu_sigma = MuSigmaEncoder(r_dim, z_dim)\n",
    "        self.xz_to_y = Decoder(x_dim, z_dim, h_dim, y_dim)\n",
    "\n",
    "    def aggregate(self, r_i):\n",
    "        return torch.mean(r_i, dim=1)\n",
    "\n",
    "    def xy_to_mu_sigma(self, x, y):\n",
    "        \"\"\"\n",
    "        Maps (x, y) pairs into the mu and sigma parameters defining the normal\n",
    "        distribution of the latent variables z.\n",
    "        \"\"\"\n",
    "        batch_size, num_points, _ = x.size()\n",
    "        x_flat = x.view(batch_size * num_points, self.x_dim)\n",
    "        y_flat = y.contiguous().view(batch_size * num_points, self.y_dim)\n",
    "        r_i_flat = self.xy_to_r(x_flat, y_flat)\n",
    "        r_i = r_i_flat.view(batch_size, num_points, self.r_dim)\n",
    "\n",
    "        # Aggregate representations r_i into a single representation r\n",
    "        r = self.aggregate(r_i)\n",
    "        return self.r_to_mu_sigma(r)\n",
    "\n",
    "    def forward(self, x, y, output_type):\n",
    "        \"\"\"\n",
    "        returns a distribution over target points y_target. We follow the convention given in \"Empirical Evaluation of Neural\n",
    "        Process Objectives\" where context is a subset of target points. This was\n",
    "        shown to work best empirically.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "\n",
    "            mu, sigma = self.xy_to_mu_sigma(x, y)\n",
    "            z_variational_posterior = Normal(mu, sigma)\n",
    "\n",
    "            z_sample = z_variational_posterior.rsample([20]) # increase the number of samples\n",
    "            p_y_pred = self.xz_to_y(x, z_sample, output_type)\n",
    "            return p_y_pred, z_variational_posterior\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RiskLearner Trainer:**\n",
    "\n",
    "- ELBO:\n",
    "$$\\max_{\\psi\\in\\Psi,\\phi\\in\\Phi}\\mathcal{G}_{\\text{ELBO}}(\\psi,\\phi):=\\mathbb{E}_{q_{\\phi}(z_{t}\\vert H_{t})}\\left[\\sum_{i=1}^{\\mathcal{B}}\\ln p_{\\psi}(\\ell_{t,i}\\vert\\tau_{t,i},z_{t})\\right]-\\beta D_{KL}\\Big[q_{\\phi}(z_{t}\\vert H_{t})\\parallel q_{\\bar{\\phi}}(z_{t}\\vert H_{t-1})\\Big]$$\n",
    "where $\\bar{\\phi}$ indicates no gradients computed through $\\phi$ in the term, and $\\{\\beta\\in\\mathbb{R}^{+},\\epsilon\\in\\mathbb{R}^{+}\\}$ constrains the machine learner's parameter search in next iteration.\n",
    "\n",
    "- Acquisition function built on the UCB principle:\n",
    "$$\n",
    "        \\mathcal{A}(\\mathcal{T}^{\\mathcal{B}};\\phi,\\psi)=\\sum_{i=1}^{\\mathcal{B}}a(\\tau_i)\n",
    "        =\\sum_{i=1}^{\\mathcal{B}}\\gamma_0\\overbrace{m(\\ell_i)}^{\\text{Risk Mean}}+\\gamma_1\\overbrace{\\sigma(\\ell_i)}^{\\text{Epistemic Uncertainty}},\n",
    "        \\\n",
    "        \\text{where}\n",
    "        \\\n",
    "        \\tau_i\\sim p(\\tau)\n",
    "        \\\\\n",
    "        \\text{with}\n",
    "        \\\n",
    "        m(\\ell_i)=\\mathbb{E}_{q_{\\phi}(z_{t}\\vert H_{t})}\\Big[p_{\\psi}(\\ell\\vert\\tau_i, z_{t})\\Big]\n",
    "        \\\n",
    "        \\text{and}\n",
    "        \\\n",
    "        \\sigma(\\ell_i)=\\mathbb{V}_{q_{\\phi}(z_{t}\\vert H_{t})}^{\\frac{1}{2}}\\Big[p_{\\psi}(\\ell\\vert\\tau_i,z_{t})\\Big],\n",
    "    $$\n",
    "where $m(\\ell_i)$ and $\\sigma(\\ell_i)$ are, respectively, the adaptation risk mean and standard deviations, which can be estimated from multiple stochastic forward passes $z_t\\sim p(z_t\\vert H_{1:t})$ and $\\ell\\sim p_{\\psi}(\\ell\\vert\\tau_{i},z_t)$ using the risk generative model.\n",
    "And $\\{\\gamma_0,\\gamma_1\\}$ are hyperparameters to balance considerations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskLearnerTrainer():\n",
    "    \"\"\"\n",
    "    Class to handle training of RiskLearner for functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, risklearner, optimizer):\n",
    "\n",
    "        self.device = device\n",
    "        self.risklearner = risklearner\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # ++++++Prediction distribution p(l|tau)++++++++++++++++++++++++++++\n",
    "        self.output_type = \"deterministic\"\n",
    "\n",
    "        # ++++++initialize the p(z_0)++++++++++++++++++++++++++++\n",
    "        r_dim = self.risklearner.r_dim\n",
    "        prior_init_mu = torch.zeros([1, r_dim]).to(self.device)\n",
    "        prior_init_sigma = torch.ones([1, r_dim]).to(self.device)\n",
    "        self.z_prior = Normal(prior_init_mu, prior_init_sigma)\n",
    "        self.last_risk_x = None\n",
    "        self.last_risk_y = None\n",
    "\n",
    "        # ++++++Acquisition functions++++++++++++++++++++++++++++\n",
    "        self.num_samples = 20\n",
    "\n",
    "    def train(self, Risk_X, Risk_Y):\n",
    "        Risk_X, Risk_Y = Risk_X.unsqueeze(0), Risk_Y.unsqueeze(0).unsqueeze(-1)\n",
    "        # shape: batch_size, num_points, dim\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        p_y_pred, z_variational_posterior = self.risklearner(Risk_X, Risk_Y, self.output_type)\n",
    "        z_prior = self.z_prior\n",
    "\n",
    "        loss = self._loss(p_y_pred, Risk_Y, z_variational_posterior, z_prior)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # updated z_prior\n",
    "        self.z_prior = Normal(z_variational_posterior.loc.detach(), z_variational_posterior.scale.detach())\n",
    "        self.last_risk_x = Risk_X\n",
    "        self.last_risk_y = Risk_Y\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def negative_log_likelihood(self, y_true, mu, sigma):\n",
    "        # Avoid division by zero\n",
    "        sigma = torch.clamp(sigma, min=1e-6)\n",
    "        # Compute the negative log likelihood\n",
    "        loss = 0.5 * torch.log(sigma ** 2) + 0.5 * ((y_true - mu) ** 2) / (sigma ** 2)\n",
    "        return loss.mean()  # taking mean over all samples if batched\n",
    "\n",
    "    def _loss(self, p_y_pred, y_target, posterior, prior):\n",
    "\n",
    "        negative_log_likelihood = F.mse_loss(p_y_pred.mean(0, keepdim=True), y_target, reduction=\"sum\")\n",
    "\n",
    "        # KL has shape (batch_size, r_dim). Take mean over batch and sum over r_dim (since r_dim is dimension of normal distribution)\n",
    "        kl = kl_divergence(posterior, prior).mean(dim=0).sum()\n",
    "\n",
    "        return negative_log_likelihood + kl\n",
    "\n",
    "    def acquisition_function(self, Risk_X_candidate, gamma_mu, gamma_sigma):\n",
    "\n",
    "        Risk_X_candidate = Risk_X_candidate.to(self.device)\n",
    "        x = Risk_X_candidate.unsqueeze(0)\n",
    "        \n",
    "        if self.last_risk_x is None:\n",
    "            z_sample = self.z_prior.rsample([self.num_samples])\n",
    "        else:\n",
    "            _, z_variational_posterior = self.risklearner(self.last_risk_x, self.last_risk_y, self.output_type)\n",
    "            z_posterior = Normal(z_variational_posterior.loc.detach(), z_variational_posterior.scale.detach())\n",
    "            z_sample = z_posterior.rsample([self.num_samples])   # sampling from prior\n",
    "        # Shape: num_samples * 1 * 10\n",
    "\n",
    "        p_y_pred = self.risklearner.xz_to_y(x, z_sample, self.output_type)\n",
    "        # Shape: num_samples * num_candidate * 1\n",
    "\n",
    "        output_mu = torch.mean(p_y_pred, dim=0)\n",
    "        output_sigma = torch.std(p_y_pred, dim=0)\n",
    "        acquisition_score = gamma_mu * output_mu + gamma_sigma * output_sigma\n",
    "        return acquisition_score, p_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "EgDOQ7h-WfID"
   },
   "outputs": [],
   "source": [
    "def single_task_test(og_net, x, y, axis, task):\n",
    "    test_alpha = 0.001\n",
    "    test_inner_steps = 10\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # modeling initialization\n",
    "    dummy_net = nn.Sequential(OrderedDict([\n",
    "        ('l1', nn.Linear(1, 40)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('l2', nn.Linear(40, 40)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('l3', nn.Linear(40, 1))]))\n",
    "    dummy_net = dummy_net.to(device)\n",
    "    dummy_net.load_state_dict(og_net.state_dict())\n",
    "    optim = torch.optim.SGD\n",
    "    opt = optim(dummy_net.parameters(), lr=test_alpha)\n",
    "\n",
    "    losses = []\n",
    "    outputs = {}\n",
    "\n",
    "    # fast adaptation for #inner_steps\n",
    "    for i in range(test_inner_steps):\n",
    "        out = dummy_net(x)\n",
    "        loss = criterion(out, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        dummy_net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Inference for the current task\n",
    "    outputs['initial'] = og_net(axis.view(-1, 1)).clone().detach().cpu().numpy()\n",
    "    outputs['adapted'] = dummy_net(axis.view(-1, 1)).clone().detach().cpu().numpy()\n",
    "    outputs[\"gt\"] = task.true_sine(axis.cpu().clone().numpy())\n",
    "\n",
    "    meta_losses = criterion(torch.tensor(outputs[\"adapted\"]), torch.tensor(outputs[\"gt\"]).unsqueeze(1))\n",
    "\n",
    "    return outputs, meta_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "fzWpT5f1Wkqp",
    "outputId": "020d4d08-b05e-48a1-964e-0fb73c585f34"
   },
   "outputs": [],
   "source": [
    "def test(tasks, og_net, k, global_seed):\n",
    "\n",
    "    # +++++++++++++++++++++\n",
    "    # Fixed Meta-Test Tasks\n",
    "    seed = 1000000\n",
    "    np.random.seed(seed)\n",
    "    num_test_tasks = 1000\n",
    "    # +++++++++++++++++++++\n",
    "    test_MSE_all = []\n",
    "\n",
    "    # Generating test task discriptors\n",
    "    test_amp_list, test_phase_list = tasks.task_descriptor_candidate(num_test_tasks)\n",
    "\n",
    "    # Testing\n",
    "    for i in range(num_test_tasks):\n",
    "        test_task = tasks.active_sample_task(test_amp_list[i], test_phase_list[i])\n",
    "        # target set\n",
    "        x, y = test_task.sample_data(k)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # query set\n",
    "        axis = np.linspace(-5, 5, 1000)\n",
    "        axis = torch.tensor(axis, dtype=torch.float)\n",
    "        axis = axis.to(device)\n",
    "\n",
    "        outputs, meta_losses = single_task_test(og_net, x, y, axis, test_task)\n",
    "        test_MSE_all.append(meta_losses)\n",
    "\n",
    "    test_MSE_all = torch.Tensor(test_MSE_all)\n",
    "    test_score = test_MSE_all.mean()\n",
    "    np.random.seed(global_seed)\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAML sine backbone\n",
    "\n",
    "https://github.com/GauravIyer/MAML-Pytorch/blob/master/Experiment%201/Experiment_1_Sine_Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "uB2cQPE9V6HO"
   },
   "outputs": [],
   "source": [
    "class SineMAML():\n",
    "    def __init__(self, sine_tasks, net, alpha, beta, k, num_candidates, num_metatasks, sampling_strategy, num_epochs, global_seed, gamma_mu, gamma_sigma):\n",
    "        self.global_seed = global_seed\n",
    "        self.device = device\n",
    "        # set the risklearner\n",
    "        risklearner = RiskLearner(x_dim=2, y_dim=1, r_dim=10, z_dim=10, h_dim=10)\n",
    "        risklearner = risklearner.to(device)\n",
    "        self.risklearner = risklearner\n",
    "        risklearner_optimizer = torch.optim.Adam(self.risklearner.parameters(), lr=5e-4)\n",
    "        self.risklearner_trainer = RiskLearnerTrainer(device, self.risklearner, risklearner_optimizer)\n",
    "        self.gamma_mu = gamma_mu\n",
    "        self.gamma_sigma = gamma_sigma\n",
    "        ######################################\n",
    "        \n",
    "        self.net=net\n",
    "        self.weights=list(net.parameters())\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        self.k=k\n",
    "        \n",
    "        # task distributions\n",
    "        self.tasks=sine_tasks\n",
    "        self.num_candidates = num_candidates\n",
    "        self.num_metatasks=num_metatasks\n",
    "        \n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        \n",
    "        self.criterion=nn.MSELoss()\n",
    "        self.meta_optimiser=torch.optim.Adam(self.weights,self.beta)\n",
    "        \n",
    "        self.meta_losses=[]\n",
    "        self.test_score_all = []\n",
    "        self.plot_every = 100\n",
    "        self.print_every = 100\n",
    "        self.test_every = 500\n",
    "        self.num_epochs = num_epochs\n",
    "  \n",
    "\n",
    "    def inner_loop(self, task):\n",
    "        temp_weights = [w.clone() for w in self.weights]\n",
    "        # Fast adaptation on the support set\n",
    "        x, y = task.sample_data(size=self.k)  # sampling D\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        output = self.net.argforward(x, temp_weights)\n",
    "        loss = self.criterion(output, y)\n",
    "        grads = torch.autograd.grad(loss, temp_weights)\n",
    "        temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # temporary update of weights\n",
    "\n",
    "        # Testing on the query set\n",
    "        x, y = task.sample_data(size=self.k)  # sampling D'\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        output = self.net.argforward(x, temp_weights)\n",
    "        metaloss = self.criterion(output, y)\n",
    "        return metaloss\n",
    "\n",
    "    def outer_loop(self):\n",
    "        total_loss=0\n",
    "        total_risklearner_loss = 0.0\n",
    "        for epoch in range(1,self.num_epochs+1):\n",
    "            metaloss_sum=0\n",
    "            metaloss_list = []\n",
    "            self.meta_optimiser.zero_grad()\n",
    "            \n",
    "            ###################################################################\n",
    "            # +++++++++++++++++++++\n",
    "            np.random.seed(epoch)\n",
    "            # +++++++++++++++++++++\n",
    "            amp_list_candidate, phase_list_candidate = self.tasks.task_descriptor_candidate(self.num_candidates)\n",
    "\n",
    "            if self.sampling_strategy == \"erm\":\n",
    "                amp_list, phase_list = amp_list_candidate[:self.num_metatasks], phase_list_candidate[:self.num_metatasks]\n",
    "\n",
    "                # innerloop: dealing with each task one by one.+++++++++++++++++++\n",
    "                for i in range(self.num_metatasks):\n",
    "                    task = self.tasks.active_sample_task(amp_list[i], phase_list[i])\n",
    "                    metaloss = self.inner_loop(task) # query loss on the task\n",
    "                    metaloss_sum += metaloss\n",
    "                    metaloss_list.append(metaloss)\n",
    "                risklearner_loss = metaloss * 0.0\n",
    "\n",
    "            elif self.sampling_strategy == \"mpts\":\n",
    "\n",
    "                Risk_X_candidate = torch.cat((torch.tensor(amp_list_candidate),torch.tensor(phase_list_candidate)), dim=1) # Shape: num_candidate * 2\n",
    "                acquisition_score, p_y_pred = self.risklearner_trainer.acquisition_function(Risk_X_candidate, self.gamma_mu, self.gamma_sigma)\n",
    "                acquisition_score = acquisition_score.squeeze(1) # Shape: num_candidate\n",
    "\n",
    "                _, selected_index = torch.topk(acquisition_score, k=self.num_candidates)\n",
    "                start_point = 0\n",
    "                selected_index = selected_index[start_point:start_point + self.num_metatasks]\n",
    "                selected_index = selected_index.cpu()\n",
    "\n",
    "                amp_list = amp_list_candidate[selected_index]\n",
    "                phase_list = phase_list_candidate[selected_index]\n",
    "\n",
    "                # innerloop: dealing with each task one by one.+++++++++++++++++++\n",
    "                for i in range(self.num_metatasks):\n",
    "                    task = self.tasks.active_sample_task(amp_list[i], phase_list[i])\n",
    "                    metaloss = self.inner_loop(task)\n",
    "                    metaloss_sum += metaloss\n",
    "                    metaloss_list.append(metaloss)\n",
    "\n",
    "                # Training Sampling Module +++++++++++++++++++++++++++++++++++++++\n",
    "                Risk_X = torch.cat((torch.tensor(amp_list), torch.tensor(phase_list)), dim=1)\n",
    "                Risk_Y = torch.tensor(metaloss_list) / self.k\n",
    "                Risk_X, Risk_Y = Risk_X.to(self.device), Risk_Y.to(self.device)\n",
    "                # Shape: num_metatasks * 2\n",
    "                # Shape: num_metatasks\n",
    "                risklearner_loss = self.risklearner_trainer.train(Risk_X, Risk_Y)\n",
    "\n",
    "            np.random.seed(self.global_seed)\n",
    "            ########################################################################\n",
    "            '''shared by all sampling strategies!'''\n",
    "            '''WITH metaloss_sum'''\n",
    "            # important step: update the backbone parameters in the last step\n",
    "            metagrads = torch.autograd.grad(metaloss_sum, self.weights)\n",
    "            for w, g in zip(self.weights, metagrads):\n",
    "                w.grad = g\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            # printing\n",
    "            total_loss += metaloss_sum.item() / self.num_metatasks\n",
    "            total_risklearner_loss += risklearner_loss.item()\n",
    "            \n",
    "            if epoch % self.print_every == 0:\n",
    "                print(\"{}/{}. loss: {}, risklearner_loss: {}\".format(epoch,\n",
    "                                                                            self.num_epochs,\n",
    "                                                                            total_loss / self.print_every,\n",
    "                                                                            total_risklearner_loss / self.print_every))\n",
    "        \n",
    "                self.meta_losses.append(total_loss/self.print_every)\n",
    "                total_loss = 0\n",
    "                total_risklearner_loss = 0\n",
    "            \n",
    "            # Testing\n",
    "            if epoch % self.test_every == 0:\n",
    "                test_score = test(self.tasks, self.net.net, self.k, self.global_seed)\n",
    "                print(\"{}/{}. test_MSE: {}\".format(epoch, self.num_epochs, test_score))\n",
    "                self.test_score_all.append(test_score)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hClUudjJWCRz",
    "outputId": "14bbc675-0b72-4c0e-fc10-20ea5ed53d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/20000. loss: 5.844093482494355, risklearner_loss: 4.106216982603073\n",
      "200/20000. loss: 5.403556756973266, risklearner_loss: 1.5598632460832595\n",
      "300/20000. loss: 5.182588694095611, risklearner_loss: 1.5581291070580483\n",
      "400/20000. loss: 4.862932562828064, risklearner_loss: 1.3449201363325118\n",
      "500/20000. loss: 4.999503259658813, risklearner_loss: 1.4135533004999161\n",
      "600/20000. loss: 4.633466455936432, risklearner_loss: 1.2961906665563583\n",
      "700/20000. loss: 4.695474369525909, risklearner_loss: 1.4640999087691307\n",
      "800/20000. loss: 4.557740080356598, risklearner_loss: 1.2589919084310532\n",
      "900/20000. loss: 4.399771273136139, risklearner_loss: 1.1158557778596878\n",
      "1000/20000. loss: 4.56640961766243, risklearner_loss: 1.1071350246667861\n",
      "1000/20000. test_MSE: 2.231252670288086\n",
      "1100/20000. loss: 4.478718659877777, risklearner_loss: 0.9736377587914467\n",
      "1200/20000. loss: 4.666054117679596, risklearner_loss: 0.8287290814518928\n",
      "1300/20000. loss: 4.530602722167969, risklearner_loss: 0.7308972437679767\n",
      "1400/20000. loss: 4.445289041996002, risklearner_loss: 0.64912471935153\n",
      "1500/20000. loss: 4.3658432149887085, risklearner_loss: 0.6148528945446015\n",
      "1600/20000. loss: 4.569287700653076, risklearner_loss: 0.7234303070604802\n",
      "1700/20000. loss: 4.369696612358093, risklearner_loss: 0.648730775564909\n",
      "1800/20000. loss: 4.2228700852394105, risklearner_loss: 0.4775177749991417\n",
      "1900/20000. loss: 4.224293689727784, risklearner_loss: 0.4551714088767767\n",
      "2000/20000. loss: 4.145850474834442, risklearner_loss: 0.3854735342413187\n",
      "2000/20000. test_MSE: 1.9757031202316284\n",
      "2100/20000. loss: 4.229651982784271, risklearner_loss: 0.39588543206453325\n",
      "2200/20000. loss: 4.209711142778397, risklearner_loss: 0.34993581131100654\n",
      "2300/20000. loss: 4.06642130613327, risklearner_loss: 0.38149539932608606\n",
      "2400/20000. loss: 4.109111649990082, risklearner_loss: 0.3867239090055227\n",
      "2500/20000. loss: 3.907127928733826, risklearner_loss: 0.3371506783366203\n",
      "2600/20000. loss: 3.981632134914398, risklearner_loss: 0.32811647914350034\n",
      "2700/20000. loss: 4.084038445949554, risklearner_loss: 0.34421445421874525\n",
      "2800/20000. loss: 4.066686856746673, risklearner_loss: 0.38845285765826704\n",
      "2900/20000. loss: 3.92639080286026, risklearner_loss: 0.3227210234850645\n",
      "3000/20000. loss: 3.8056713271141054, risklearner_loss: 0.35574735268950464\n",
      "3000/20000. test_MSE: 1.8623594045639038\n",
      "3100/20000. loss: 3.7588001227378847, risklearner_loss: 0.36925639666616916\n",
      "3200/20000. loss: 3.935875928401947, risklearner_loss: 0.3952803239226341\n",
      "3300/20000. loss: 3.827362308502197, risklearner_loss: 0.34002686813473704\n",
      "3400/20000. loss: 3.7849304389953615, risklearner_loss: 0.3083936386555433\n",
      "3500/20000. loss: 3.7263952231407167, risklearner_loss: 0.3254990137368441\n",
      "3600/20000. loss: 3.6433026003837585, risklearner_loss: 0.33422253593802453\n",
      "3700/20000. loss: 3.667970142364502, risklearner_loss: 0.30312724199146035\n",
      "3800/20000. loss: 3.680119664669037, risklearner_loss: 0.35264211282134056\n",
      "3900/20000. loss: 3.6687908577919006, risklearner_loss: 0.34411588467657567\n",
      "4000/20000. loss: 3.561963219642639, risklearner_loss: 0.32783068660646675\n",
      "4000/20000. test_MSE: 1.6476690769195557\n",
      "4100/20000. loss: 3.728881494998932, risklearner_loss: 0.3592152826860547\n",
      "4200/20000. loss: 3.435209906101227, risklearner_loss: 0.28071477968245745\n",
      "4300/20000. loss: 3.4877204418182375, risklearner_loss: 0.2842999168485403\n",
      "4400/20000. loss: 3.383213939666748, risklearner_loss: 0.2994340289011598\n",
      "4500/20000. loss: 3.4019927883148195, risklearner_loss: 0.3663643515482545\n",
      "4600/20000. loss: 3.3467079889774323, risklearner_loss: 0.2733443292975426\n",
      "4700/20000. loss: 3.273361556529999, risklearner_loss: 0.27520919919013975\n",
      "4800/20000. loss: 3.2129810655117037, risklearner_loss: 0.28884454641491175\n",
      "4900/20000. loss: 3.168520123958588, risklearner_loss: 0.274517904818058\n",
      "5000/20000. loss: 3.0716485702991485, risklearner_loss: 0.2574494722113013\n",
      "5000/20000. test_MSE: 1.3659268617630005\n",
      "5100/20000. loss: 3.0260047912597656, risklearner_loss: 0.2697636516019702\n",
      "5200/20000. loss: 3.0088671588897706, risklearner_loss: 0.2562241629883647\n",
      "5300/20000. loss: 2.896117105484009, risklearner_loss: 0.21241771772503854\n",
      "5400/20000. loss: 2.8008197486400603, risklearner_loss: 0.24224386483430863\n",
      "5500/20000. loss: 2.7736211895942686, risklearner_loss: 0.22139024131000043\n",
      "5600/20000. loss: 2.6500163769721983, risklearner_loss: 0.2017400858923793\n",
      "5700/20000. loss: 2.531778337955475, risklearner_loss: 0.20785886038094759\n",
      "5800/20000. loss: 2.465986657142639, risklearner_loss: 0.1973181040212512\n",
      "5900/20000. loss: 2.407916855812073, risklearner_loss: 0.19732633534818889\n",
      "6000/20000. loss: 2.328908749818802, risklearner_loss: 0.18548874486237765\n",
      "6000/20000. test_MSE: 1.0235497951507568\n",
      "6100/20000. loss: 2.2302542614936827, risklearner_loss: 0.17551688570529222\n",
      "6200/20000. loss: 2.149126105308533, risklearner_loss: 0.15051856193691493\n",
      "6300/20000. loss: 2.1498618650436403, risklearner_loss: 0.15954423278570176\n",
      "6400/20000. loss: 2.0788865995407106, risklearner_loss: 0.1643667459115386\n",
      "6500/20000. loss: 2.0226897239685058, risklearner_loss: 0.15822640843689442\n",
      "6600/20000. loss: 1.9572950494289398, risklearner_loss: 0.17050651971250771\n",
      "6700/20000. loss: 1.9564644372463227, risklearner_loss: 0.20389479249715806\n",
      "6800/20000. loss: 1.8984498155117036, risklearner_loss: 0.16849215345457197\n",
      "6900/20000. loss: 1.8689298164844512, risklearner_loss: 0.15222469061613084\n",
      "7000/20000. loss: 1.8260413265228272, risklearner_loss: 0.16876416072249412\n",
      "7000/20000. test_MSE: 0.6809158325195312\n",
      "7100/20000. loss: 1.727730129957199, risklearner_loss: 0.13916557345539332\n",
      "7200/20000. loss: 1.7287511122226715, risklearner_loss: 0.1224671258032322\n",
      "7300/20000. loss: 1.7683394348621368, risklearner_loss: 0.1647393399104476\n",
      "7400/20000. loss: 1.683601975440979, risklearner_loss: 0.13110124060884118\n",
      "7500/20000. loss: 1.6404472881555556, risklearner_loss: 0.12113141579553485\n",
      "7600/20000. loss: 1.620566794872284, risklearner_loss: 0.10979158490896225\n",
      "7700/20000. loss: 1.589822359085083, risklearner_loss: 0.12893317487090827\n",
      "7800/20000. loss: 1.571638633608818, risklearner_loss: 0.13813392162323\n",
      "7900/20000. loss: 1.5717537009716034, risklearner_loss: 0.1219998143054545\n",
      "8000/20000. loss: 1.552446825504303, risklearner_loss: 0.11532108468934893\n",
      "8000/20000. test_MSE: 0.5028948187828064\n",
      "8100/20000. loss: 1.481313207745552, risklearner_loss: 0.1290537129715085\n",
      "8200/20000. loss: 1.5572523599863053, risklearner_loss: 0.13208130737766624\n",
      "8300/20000. loss: 1.4771085685491563, risklearner_loss: 0.12070860208943486\n",
      "8400/20000. loss: 1.5375701308250427, risklearner_loss: 0.1455273725092411\n",
      "8500/20000. loss: 1.4825489658117295, risklearner_loss: 0.12071632221341133\n",
      "8600/20000. loss: 1.4610131812095641, risklearner_loss: 0.12189156901091337\n",
      "8700/20000. loss: 1.4469697898626328, risklearner_loss: 0.12581339539028705\n",
      "8800/20000. loss: 1.4468548339605332, risklearner_loss: 0.11448051877319813\n",
      "8900/20000. loss: 1.422631396651268, risklearner_loss: 0.13800269581377506\n",
      "9000/20000. loss: 1.421811101436615, risklearner_loss: 0.12032047770917416\n",
      "9000/20000. test_MSE: 0.42218270897865295\n",
      "9100/20000. loss: 1.4488804799318313, risklearner_loss: 0.12243053637444973\n",
      "9200/20000. loss: 1.3625978177785874, risklearner_loss: 0.10492481322959066\n",
      "9300/20000. loss: 1.4333534568548203, risklearner_loss: 0.14812790477648377\n",
      "9400/20000. loss: 1.3476350855827333, risklearner_loss: 0.10878055945038795\n",
      "9500/20000. loss: 1.3698202520608902, risklearner_loss: 0.11496742447838187\n",
      "9600/20000. loss: 1.3749321597814559, risklearner_loss: 0.1599687465839088\n",
      "9700/20000. loss: 1.331165012717247, risklearner_loss: 0.10833631648682057\n",
      "9800/20000. loss: 1.3159987568855285, risklearner_loss: 0.11211676204577088\n",
      "9900/20000. loss: 1.3092428773641587, risklearner_loss: 0.12797227645292877\n",
      "10000/20000. loss: 1.2956132280826569, risklearner_loss: 0.10049778688699007\n",
      "10000/20000. test_MSE: 0.37166285514831543\n",
      "10100/20000. loss: 1.3296009331941605, risklearner_loss: 0.13922602163627742\n",
      "10200/20000. loss: 1.3626581233739854, risklearner_loss: 0.11064786920323968\n",
      "10300/20000. loss: 1.3421285218000412, risklearner_loss: 0.12617590976879\n",
      "10400/20000. loss: 1.3383148020505906, risklearner_loss: 0.1260876346193254\n",
      "10500/20000. loss: 1.2966960179805755, risklearner_loss: 0.11401931902393699\n",
      "10600/20000. loss: 1.3095441192388535, risklearner_loss: 0.11078531555831432\n",
      "10700/20000. loss: 1.2827155900001526, risklearner_loss: 0.0997327834367752\n",
      "10800/20000. loss: 1.3163493585586548, risklearner_loss: 0.12944060243666172\n",
      "10900/20000. loss: 1.2413501411676406, risklearner_loss: 0.09624347299337387\n",
      "11000/20000. loss: 1.285109176635742, risklearner_loss: 0.10330084532499313\n",
      "11000/20000. test_MSE: 0.3309824764728546\n",
      "11100/20000. loss: 1.2961253374814987, risklearner_loss: 0.10722343739122152\n",
      "11200/20000. loss: 1.307970860004425, risklearner_loss: 0.11502830412238836\n",
      "11300/20000. loss: 1.2478057086467742, risklearner_loss: 0.10476282482966781\n",
      "11400/20000. loss: 1.251075161099434, risklearner_loss: 0.10171861469745636\n",
      "11500/20000. loss: 1.2631749391555787, risklearner_loss: 0.09751986522227525\n",
      "11600/20000. loss: 1.257584243416786, risklearner_loss: 0.10750264044851064\n",
      "11700/20000. loss: 1.2067264300584792, risklearner_loss: 0.0938309358805418\n",
      "11800/20000. loss: 1.2510272091627122, risklearner_loss: 0.10826101744547487\n",
      "11900/20000. loss: 1.2350418418645859, risklearner_loss: 0.10606152739375829\n",
      "12000/20000. loss: 1.2814221090078355, risklearner_loss: 0.11319851389154792\n",
      "12000/20000. test_MSE: 0.33071231842041016\n",
      "12100/20000. loss: 1.2466849678754806, risklearner_loss: 0.09977126849815249\n",
      "12200/20000. loss: 1.253512099981308, risklearner_loss: 0.10228645067662001\n",
      "12300/20000. loss: 1.2233941894769669, risklearner_loss: 0.09603017637506127\n",
      "12400/20000. loss: 1.250747801065445, risklearner_loss: 0.10958433767780662\n",
      "12500/20000. loss: 1.2311037456989289, risklearner_loss: 0.09645942885428667\n",
      "12600/20000. loss: 1.263115450143814, risklearner_loss: 0.10666254878044129\n",
      "12700/20000. loss: 1.191124315261841, risklearner_loss: 0.0940464728884399\n",
      "12800/20000. loss: 1.2286242741346358, risklearner_loss: 0.09925489718094468\n",
      "12900/20000. loss: 1.229078670144081, risklearner_loss: 0.10761221850290895\n",
      "13000/20000. loss: 1.2085547679662705, risklearner_loss: 0.09950848054140807\n",
      "13000/20000. test_MSE: 0.2961033582687378\n",
      "13100/20000. loss: 1.2432249420881272, risklearner_loss: 0.0970905539393425\n",
      "13200/20000. loss: 1.2164181524515152, risklearner_loss: 0.10868939310312271\n",
      "13300/20000. loss: 1.2271692472696305, risklearner_loss: 0.10983964651823044\n",
      "13400/20000. loss: 1.198050658106804, risklearner_loss: 0.10481011187657714\n",
      "13500/20000. loss: 1.2287248373031616, risklearner_loss: 0.11718525014817714\n",
      "13600/20000. loss: 1.2269358414411544, risklearner_loss: 0.1078348122164607\n",
      "13700/20000. loss: 1.2392043745517731, risklearner_loss: 0.09755597420036793\n",
      "13800/20000. loss: 1.215387688279152, risklearner_loss: 0.11119996417313814\n",
      "13900/20000. loss: 1.178273494243622, risklearner_loss: 0.10643570091575384\n",
      "14000/20000. loss: 1.225122607946396, risklearner_loss: 0.11786167407408357\n",
      "14000/20000. test_MSE: 0.28547045588493347\n",
      "14100/20000. loss: 1.210905550122261, risklearner_loss: 0.104957281537354\n",
      "14200/20000. loss: 1.223027856349945, risklearner_loss: 0.11707761734724045\n",
      "14300/20000. loss: 1.1943194013834, risklearner_loss: 0.09735669868066907\n",
      "14400/20000. loss: 1.2190998435020446, risklearner_loss: 0.12057092800736427\n",
      "14500/20000. loss: 1.2036829406023026, risklearner_loss: 0.09703230362385512\n",
      "14600/20000. loss: 1.2017009198665618, risklearner_loss: 0.10898082656785846\n",
      "14700/20000. loss: 1.155410767197609, risklearner_loss: 0.0986534732952714\n",
      "14800/20000. loss: 1.2145225948095322, risklearner_loss: 0.11504734283313155\n",
      "14900/20000. loss: 1.2239508461952209, risklearner_loss: 0.10403196794912219\n",
      "15000/20000. loss: 1.1962727546691894, risklearner_loss: 0.10898252596147358\n",
      "15000/20000. test_MSE: 0.25992026925086975\n",
      "15100/20000. loss: 1.1589875417947768, risklearner_loss: 0.10848248617723584\n",
      "15200/20000. loss: 1.1819921106100082, risklearner_loss: 0.1017483427748084\n",
      "15300/20000. loss: 1.2294864857196808, risklearner_loss: 0.11496104640886187\n",
      "15400/20000. loss: 1.1784674590826034, risklearner_loss: 0.09980432290583849\n",
      "15500/20000. loss: 1.2128736746311188, risklearner_loss: 0.1138099804520607\n",
      "15600/20000. loss: 1.185216143131256, risklearner_loss: 0.11133381580933928\n",
      "15700/20000. loss: 1.1370093631744385, risklearner_loss: 0.08995960053056479\n",
      "15800/20000. loss: 1.1938320761919021, risklearner_loss: 0.10493035810068249\n",
      "15900/20000. loss: 1.1796668457984925, risklearner_loss: 0.10877395052462817\n",
      "16000/20000. loss: 1.1243316334486009, risklearner_loss: 0.09127109475433827\n",
      "16000/20000. test_MSE: 0.2630755603313446\n",
      "16100/20000. loss: 1.187576369047165, risklearner_loss: 0.11202078092843294\n",
      "16200/20000. loss: 1.1637893450260162, risklearner_loss: 0.10813483107835055\n",
      "16300/20000. loss: 1.182342960834503, risklearner_loss: 0.10093024112284184\n",
      "16400/20000. loss: 1.1672950667142867, risklearner_loss: 0.09687580307945609\n",
      "16500/20000. loss: 1.1859834611415863, risklearner_loss: 0.11132457887753844\n",
      "16600/20000. loss: 1.1625298446416854, risklearner_loss: 0.10143173787742853\n",
      "16700/20000. loss: 1.155274249315262, risklearner_loss: 0.0915306837297976\n",
      "16800/20000. loss: 1.071845988035202, risklearner_loss: 0.07723187679424882\n",
      "16900/20000. loss: 1.1529942405223848, risklearner_loss: 0.09583365932106971\n",
      "17000/20000. loss: 1.1359379935264586, risklearner_loss: 0.09564275979995727\n",
      "17000/20000. test_MSE: 0.2431544065475464\n",
      "17100/20000. loss: 1.1956553894281388, risklearner_loss: 0.0995396595261991\n",
      "17200/20000. loss: 1.1769964879751205, risklearner_loss: 0.09935717854648829\n",
      "17300/20000. loss: 1.1117669093608855, risklearner_loss: 0.09035488180816173\n",
      "17400/20000. loss: 1.1679990273714065, risklearner_loss: 0.10083872521296143\n",
      "17500/20000. loss: 1.1232962268590927, risklearner_loss: 0.09046456830576062\n",
      "17600/20000. loss: 1.0972854560613632, risklearner_loss: 0.08513369854539633\n",
      "17700/20000. loss: 1.1510934138298035, risklearner_loss: 0.10290039423853159\n",
      "17800/20000. loss: 1.101843992471695, risklearner_loss: 0.09379998408257961\n",
      "17900/20000. loss: 1.1067807948589325, risklearner_loss: 0.09385949477553368\n",
      "18000/20000. loss: 1.0702787667512894, risklearner_loss: 0.08599911486729979\n",
      "18000/20000. test_MSE: 0.22957965731620789\n",
      "18100/20000. loss: 1.1293810719251633, risklearner_loss: 0.10396562453359365\n",
      "18200/20000. loss: 1.0909182292222976, risklearner_loss: 0.08465720230713486\n",
      "18300/20000. loss: 1.160391672849655, risklearner_loss: 0.11304709793999791\n",
      "18400/20000. loss: 1.1064893233776092, risklearner_loss: 0.09117501052096486\n",
      "18500/20000. loss: 1.0684283649921418, risklearner_loss: 0.07855143395252526\n",
      "18600/20000. loss: 1.0406007444858552, risklearner_loss: 0.0812747988384217\n",
      "18700/20000. loss: 1.0813022708892823, risklearner_loss: 0.09153893768787384\n",
      "18800/20000. loss: 1.0782075697183608, risklearner_loss: 0.08763331579044462\n",
      "18900/20000. loss: 1.0810344529151916, risklearner_loss: 0.10583007015287876\n",
      "19000/20000. loss: 1.0882366621494293, risklearner_loss: 0.09654977200552821\n",
      "19000/20000. test_MSE: 0.2250770926475525\n",
      "19100/20000. loss: 1.0985442727804184, risklearner_loss: 0.07827757833525538\n",
      "19200/20000. loss: 1.119390714764595, risklearner_loss: 0.09719134341925383\n",
      "19300/20000. loss: 1.0512249648571015, risklearner_loss: 0.07492644093930721\n",
      "19400/20000. loss: 1.1112492841482162, risklearner_loss: 0.10993464203551412\n",
      "19500/20000. loss: 1.090249925851822, risklearner_loss: 0.09488307978957891\n",
      "19600/20000. loss: 1.124843448996544, risklearner_loss: 0.09243609916418791\n",
      "19700/20000. loss: 1.0808332133293153, risklearner_loss: 0.08231788927689195\n",
      "19800/20000. loss: 1.0996652978658676, risklearner_loss: 0.09024919079616665\n",
      "19900/20000. loss: 1.0835798239707948, risklearner_loss: 0.09066836677491664\n",
      "20000/20000. loss: 1.1253045427799224, risklearner_loss: 0.09507675688713789\n",
      "20000/20000. test_MSE: 0.22143778204917908\n"
     ]
    }
   ],
   "source": [
    "# set the task distribution+++++++++++++++++++++++++++++++++++++++++++++++\n",
    "sine_tasks=SineDistribution(0.1, 5, 0, np.pi, -5, 5)\n",
    "\n",
    "\n",
    "# set the backbone++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "net=SineNet()\n",
    "net=net.to(device)\n",
    "\n",
    "sampling_strategy = \"mpts\"\n",
    "num_candidates = 32\n",
    "num_epochs = 20000\n",
    "global_seed = 1\n",
    "\n",
    "# set the core algorithm++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "maml=SineMAML(sine_tasks=sine_tasks, net=net, alpha=0.001,beta=0.001,\n",
    "              k=10, num_candidates=num_candidates, num_metatasks=16, \n",
    "              sampling_strategy=sampling_strategy, num_epochs=num_epochs, global_seed=global_seed,\n",
    "              gamma_mu=1, gamma_sigma=3)\n",
    "\n",
    "# start training++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "maml.plot_every = 100\n",
    "maml.print_every = 100\n",
    "maml.test_every = 1000\n",
    "\n",
    "maml.meta_losses=[]\n",
    "maml.outer_loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "GrHh3US3WM5y",
    "outputId": "d90f74ae-de5c-4139-a7bc-d88c6a518474"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1, num_epochs+1, len(maml.meta_losses)+1)[:len(maml.meta_losses)], maml.meta_losses)\n",
    "plt.plot(np.linspace(1, num_epochs+1, len(maml.test_score_all)+1)[:len(maml.test_score_all)]+maml.test_every, maml.test_score_all)\n",
    "plt.title(sampling_strategy+\"__MSE on fixed 100 tasks\")\n",
    "plt.ylim((0, 7))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
